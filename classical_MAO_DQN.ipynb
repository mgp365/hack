{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1fabd9b",
   "metadata": {},
   "source": [
    "# MAO Reinforcement Learning – **Classical DQN**\n",
    "This notebook contains a fully‑working reference implementation of the MAO environment, two rule‑based agents, and a Deep Q‑Network learner based on PyTorch.  Run all cells top‑to‑bottom to train the agent; adjust hyper‑parameters at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "id": "d8caa837",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T22:00:16.571222Z",
     "start_time": "2025-06-21T22:00:15.513715Z"
    }
   },
   "source": [
    "import random, math, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "# ---- Card helpers --------------------------------------------------\n",
    "SUITS = \"♠♥♦♣\"\n",
    "RANKS = \"A23456789TJQK\"\n",
    "CARD2IDX = {f\"{r}{s}\": i for i, (r, s) in enumerate((r, s) for r in RANKS for s in SUITS)}\n",
    "IDX2CARD = {v: k for k, v in CARD2IDX.items()}\n",
    "\n",
    "def rank(idx): return RANKS[idx // 4]\n",
    "def suit(idx): return SUITS[idx % 4]\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "23fc1215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T22:00:19.351747Z",
     "start_time": "2025-06-21T22:00:19.343620Z"
    }
   },
   "source": [
    "# ---- MAO Environment ---------------------------------------------\n",
    "class MaoEnv:\n",
    "    def __init__(self, n_players=3, penalty_draw=1):\n",
    "        self.n_players, self.penalty_draw = n_players, penalty_draw\n",
    "        self.state_size = 110\n",
    "        self.action_space = 52  # play card idx or draw (we'll treat 'draw' as action=52)\n",
    "        self.reset()\n",
    "    # ...............................................................\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None: random.seed(seed)\n",
    "        deck = list(range(52)); random.shuffle(deck)\n",
    "        self.hands = [deque(deck[i*7:(i+1)*7]) for i in range(self.n_players)]\n",
    "        self.draw_pile = deque(deck[7*self.n_players:-1])\n",
    "        self.discard   = [deck[-1]]\n",
    "        self.current_player = 0\n",
    "        self.direction = 1\n",
    "        self.skip_next = False\n",
    "        return self._obs()\n",
    "    # ...............................................................\n",
    "    def _obs(self):\n",
    "        pid = self.current_player\n",
    "        hand_vec = np.zeros(52); hand_vec[list(self.hands[pid])] = 1\n",
    "        top_vec  = np.zeros(52); top_vec[self.discard[-1]] = 1\n",
    "        counts   = np.array([len(h)/20 for i,h in enumerate(self.hands) if i!=pid])\n",
    "        turn     = np.eye(self.n_players)[pid]\n",
    "        dirflag  = np.array([int(self.direction==-1)])\n",
    "        return np.concatenate([hand_vec, top_vec, counts, turn, dirflag])\n",
    "    # ...............................................................\n",
    "    def legal_actions(self, pid):\n",
    "        top = self.discard[-1]\n",
    "        return [c for c in self.hands[pid] if rank(c)==rank(top) or suit(c)==suit(top)]\n",
    "    # ...............................................................\n",
    "    def step(self, action):\n",
    "        reward, done = 0.0, False\n",
    "        pid = self.current_player\n",
    "        if action == 52 or action not in self.legal_actions(pid):\n",
    "            self._penalize(pid)\n",
    "            reward = -1\n",
    "        else:\n",
    "            self.hands[pid].remove(action)\n",
    "            self.discard.append(action)\n",
    "            reward = +1\n",
    "            r = rank(action)\n",
    "            if r == 'A': self.skip_next = True\n",
    "            elif r == 'K': self.direction *= -1\n",
    "            if len(self.hands[pid]) == 0:\n",
    "                done, reward = True, 10\n",
    "        if not done:\n",
    "            self._advance()\n",
    "        return self._obs(), reward, done, {}\n",
    "    # ...............................................................\n",
    "    def _penalize(self, pid, n=None):\n",
    "        n = n or self.penalty_draw\n",
    "        for _ in range(n):\n",
    "            if self.draw_pile: self.hands[pid].append(self.draw_pile.popleft())\n",
    "    # ...............................................................\n",
    "    def _advance(self):\n",
    "        step = self.direction\n",
    "        nxt  = (self.current_player + step) % self.n_players\n",
    "        if self.skip_next:\n",
    "            nxt = (nxt + step) % self.n_players\n",
    "            self.skip_next = False\n",
    "        self.current_player = nxt\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1d952198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T22:00:20.845187Z",
     "start_time": "2025-06-21T22:00:20.841852Z"
    }
   },
   "source": [
    "# ---- Replay Buffer ------------------------------------------------\n",
    "Transition = namedtuple('T', ('s','a','r','s2','d'))\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, cap=100_000): self.cap=cap; self.buf=deque(maxlen=cap)\n",
    "    def push(self,*args): self.buf.append(Transition(*args))\n",
    "    def sample(self, k):  return random.sample(self.buf, k)\n",
    "    def __len__(self):    return len(self.buf)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "4c3ad768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T22:00:21.689878Z",
     "start_time": "2025-06-21T22:00:21.683888Z"
    }
   },
   "source": [
    "# ---- DQN Agent ----------------------------------------------------\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, state, actions):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state,256)\n",
    "        self.fc2 = nn.Linear(256,256)\n",
    "        self.out = nn.Linear(256,actions)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, lr=1e-4, gamma=0.99,\n",
    "                 eps_start=1.0, eps_end=0.1, eps_decay=5e-5):\n",
    "        self.q = QNet(state_size, action_size)\n",
    "        self.target = QNet(state_size, action_size)\n",
    "        self.target.load_state_dict(self.q.state_dict())\n",
    "        self.opt = torch.optim.Adam(self.q.parameters(), lr=lr)\n",
    "        self.mem = ReplayBuffer()\n",
    "        self.gamma, self.eps, self.eps_end, self.eps_decay = gamma, eps_start, eps_end, eps_decay\n",
    "        self.steps = 0\n",
    "    # ...............................................................\n",
    "    def act(self, obs, legal):\n",
    "        self.steps += 1\n",
    "        self.eps = max(self.eps_end, self.eps - self.eps_decay)\n",
    "        if random.random() < self.eps:\n",
    "            return random.choice(legal) if legal else 52\n",
    "        with torch.no_grad():\n",
    "            qvals = self.q(torch.tensor(obs, dtype=torch.float32))\n",
    "            # mask illegal\n",
    "            mask = torch.full_like(qvals, -1e9)\n",
    "            mask[legal] = 0\n",
    "            qvals = qvals + mask\n",
    "            return int(torch.argmax(qvals).item())\n",
    "    # ...............................................................\n",
    "    def learn(self, batch=256):\n",
    "        if len(self.mem) < batch: return\n",
    "        tr = Transition(*zip(*self.mem.sample(batch)))\n",
    "        s  = torch.tensor(np.stack(tr.s), dtype=torch.float32)\n",
    "        a  = torch.tensor(tr.a).unsqueeze(1)\n",
    "        r  = torch.tensor(tr.r, dtype=torch.float32)\n",
    "        s2 = torch.tensor(np.stack(tr.s2), dtype=torch.float32)\n",
    "        d  = torch.tensor(tr.d, dtype=torch.float32)\n",
    "        q_pred = self.q(s).gather(1,a).squeeze()\n",
    "        with torch.no_grad():\n",
    "            q_next = self.target(s2).max(1)[0]\n",
    "            y = r + self.gamma * q_next * (1-d)\n",
    "        loss = F.mse_loss(q_pred, y)\n",
    "        self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
    "        if self.steps % 500 == 0:\n",
    "            self.target.load_state_dict(self.q.state_dict())\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "6551dcf9",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-21T22:00:22.479461Z"
    }
   },
   "source": [
    "# ---- Training Loop -----------------------------------------------\n",
    "env = MaoEnv()\n",
    "agent = DQNAgent(env.state_size, 53)   # 52 cards + draw\n",
    "EPISODES = 5_000\n",
    "for ep in range(EPISODES):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        legal = env.legal_actions(env.current_player)\n",
    "        action = agent.act(obs, legal)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        agent.mem.push(obs, action, reward, next_obs, done)\n",
    "        agent.learn()\n",
    "        obs = next_obs\n",
    "    if ep % 250 == 0: print(f\"Episode {ep}, ε={agent.eps:.2f}\")\n",
    "print(\"Training finished.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, ε=1.00\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9c9b04424494afd7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
